{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from training import *\n",
    "from models import *\n",
    "from A2C_agent import *\n",
    "from helpers import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 finished after 12 steps with reward 12.00\n",
      "Episode 100 finished after 12 steps with reward 12.00\n",
      "Episode 200 finished after 12 steps with reward 12.00\n",
      "Episode 300 finished after 10 steps with reward 10.00\n",
      "Episode 400 finished after 85 steps with reward 85.00\n",
      "Episode 500 finished after 37 steps with reward 37.00\n",
      "Episode 600 finished after 10 steps with reward 10.00\n",
      "Episode 700 finished after 373 steps with reward 373.00\n",
      "Episode 800 finished after 163 steps with reward 163.00\n",
      "Episode 900 finished after 274 steps with reward 274.00\n",
      "Episode 1000 finished after 137 steps with reward 137.00\n",
      "Episode 1100 finished after 21 steps with reward 21.00\n",
      "Episode 1200 finished after 73 steps with reward 73.00\n",
      "Episode 1300 finished after 52 steps with reward 52.00\n",
      "Episode 1400 finished after 77 steps with reward 77.00\n",
      "Episode 1500 finished after 181 steps with reward 181.00\n",
      "Episode 1600 finished after 139 steps with reward 139.00\n",
      "Episode 1700 finished after 165 steps with reward 165.00\n",
      "Episode 1800 finished after 15 steps with reward 15.00\n",
      "Episode 1900 finished after 17 steps with reward 17.00\n",
      "Episode 2000 finished after 14 steps with reward 14.00\n",
      "Episode 2100 finished after 88 steps with reward 88.00\n",
      "Episode 2200 finished after 56 steps with reward 56.00\n",
      "Episode 2300 finished after 109 steps with reward 109.00\n",
      "Episode 2400 finished after 175 steps with reward 175.00\n",
      "Episode 2500 finished after 133 steps with reward 133.00\n",
      "Episode 2600 finished after 164 steps with reward 164.00\n",
      "Episode 2700 finished after 61 steps with reward 61.00\n",
      "Episode 2800 finished after 189 steps with reward 189.00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[168], line 48\u001b[0m\n\u001b[1;32m     44\u001b[0m batch\u001b[38;5;241m.\u001b[39mappend((state, action, reward, next_state, done))\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(batch) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size \u001b[38;5;129;01mor\u001b[39;00m done:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m# Train the agent\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m     worker_losses \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr_actor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr_critic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# Clear the batch\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     batch\u001b[38;5;241m.\u001b[39mclear()\n",
      "File \u001b[0;32m~/Desktop/ma4/annrl/annrl_project/A2C_agent.py:61\u001b[0m, in \u001b[0;36mAgent.train\u001b[0;34m(self, batch, gamma_, lr_actor, lr_critic, device)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03mtrain all instances of worker networks (actors and critics) on a batch of experiences\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03mreturn: worker_losses (dictionary)\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m worker_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactors\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# critic_loss, actor_loss = self.train_worker(batch, worker_id, gamma_, lr)\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m     worker_losses \u001b[38;5;241m=\u001b[39m \u001b[43m{\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_worker\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworker_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_actor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_critic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_workers\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m worker_losses\n",
      "File \u001b[0;32m~/Desktop/ma4/annrl/annrl_project/A2C_agent.py:61\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03mtrain all instances of worker networks (actors and critics) on a batch of experiences\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03mreturn: worker_losses (dictionary)\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m worker_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactors\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# critic_loss, actor_loss = self.train_worker(batch, worker_id, gamma_, lr)\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m     worker_losses \u001b[38;5;241m=\u001b[39m {i: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_worker\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworker_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_actor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_critic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_workers)}\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m worker_losses\n",
      "File \u001b[0;32m~/Desktop/ma4/annrl/annrl_project/A2C_agent.py:50\u001b[0m, in \u001b[0;36mAgent.train_worker\u001b[0;34m(self, batch, worker_id, gamma_, lr_actor, lr_critic, device)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_worker\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, worker_id, gamma_, lr_actor, lr_critic, device):\n\u001b[1;32m     46\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;124;03m    train one instance of actor and critic networks on a batch of experiences\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124;03m    return: critic_loss, actor_loss\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     critic_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_critic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcritics\u001b[49m\u001b[43m[\u001b[49m\u001b[43mworker_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_critic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     actor_loss \u001b[38;5;241m=\u001b[39m train_actor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritics[worker_id], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactors[worker_id], batch, gamma_, lr_actor, device)\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m critic_loss, actor_loss\n",
      "File \u001b[0;32m~/Desktop/ma4/annrl/annrl_project/training.py:31\u001b[0m, in \u001b[0;36mtrain_critic\u001b[0;34m(critic, batch, gamma_, lr, device)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Gradient descent for the critic\u001b[39;00m\n\u001b[1;32m     30\u001b[0m critic_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 31\u001b[0m \u001b[43mcritic_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m critic_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m critic_loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/Desktop/ma4/annrl/venv4annrl/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/ma4/annrl/venv4annrl/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = device_selection() # mps -> cuda -> cpu\n",
    "\n",
    "# Initialize environment\n",
    "env = gym.make('CartPole-v1') \n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 256\n",
    "gamma_ = 0.99\n",
    "lr_actor = 1e-5\n",
    "lr_critic = 1e-3\n",
    "eps = 0.1\n",
    "num_workers=1\n",
    "num_episodes = 10000\n",
    "total_steps_budget = 500000\n",
    "max_steps_per_episode = 1000\n",
    "\n",
    "# neural network structure\n",
    "input_size = env.observation_space.shape[0] # 4\n",
    "hidden_size = 64\n",
    "output_size_actor = env.action_space.n # 2\n",
    "output_size_critic = 1\n",
    "\n",
    "\n",
    "# Initialize agent \n",
    "agent = Agent(input_size, hidden_size, \\\n",
    "                output_size_actor, output_size_critic, \\\n",
    "                eps, gamma_, lr_actor, lr_critic, num_workers, \\\n",
    "                device=device)\n",
    "\n",
    "# Initialize batch\n",
    "batch = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    state = torch.from_numpy(state).float().to(device)  # Convert state to a tensor\n",
    "\n",
    "    episode_reward = 0\n",
    "\n",
    "    for t in range(max_steps_per_episode):\n",
    "        action = agent.select_action(state, worker_id=0, policy=\"eps-greedy\")\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        agent.num_steps += 1\n",
    "        \n",
    "        next_state = torch.from_numpy(next_state).float().to(device)  # Convert next_state to a tensor\n",
    "        done = terminated or truncated\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Add the experience to the batch\n",
    "        batch.append((state, action, reward, next_state, done))\n",
    "\n",
    "        if len(batch) >= batch_size or done:\n",
    "            # Train the agent\n",
    "            worker_losses = agent.train(batch, agent.gamma, agent.lr_actor, agent.lr_critic, agent.device)\n",
    "\n",
    "            # Clear the batch\n",
    "            batch.clear()\n",
    "\n",
    "        state = next_state\n",
    "        if done: break\n",
    "\n",
    "    if episode % 100 == 0:\n",
    "        print(f\"Episode {episode} finished after {t+1} steps with reward {episode_reward:.2f}\")\n",
    "    \n",
    "    if (agent.num_steps >= total_steps_budget): \n",
    "        print(f\"Reached total training budget of {total_steps_budget} steps ----> Stopping training at episode {episode}\")\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO IMPLEMENT:\n",
    "\n",
    "- fix training for multiple workers \n",
    "    - wrong computations for target and advantage probably!\n",
    "    - wrong way to accumulate batches for K>1 right now (right now it's a replay buffer for current agent, should be transitions from K-workers, each worker on different seed of environment in parallel, all have the same networks --> no need to initialize K actor-critic networks?)\n",
    "    - mini-batch of 256 consequent transitions? wrong\n",
    "- methods for plotting and training observation (cf 2.3 of project PDF)\n",
    "- agent class needs method for saving trained models \n",
    "- add counting steps to agent class\n",
    "- output size of policy make dependent on env (not =2 constant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "workers all use the same parameters and get updated with the same gradient at the same time (if not at the same time --> updates with different policies by definition, but A2C is on-policy). Doesn't matter if they (inevitably) end episodes at different time-steps. What matters is they have the same policy when accumulating a batch (same actor-critic params)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batches vanilla are (1x1), in general (num_time_steps x num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv4annrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
