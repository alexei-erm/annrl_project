{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from training import *\n",
    "from models import *\n",
    "from A2C_agent import *\n",
    "from helpers import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 finished after 10 steps with reward 10.00\n",
      "Actor loss: 0.68, Critic loss: 1.02   Total steps: 10\n",
      "--------------------------------------------------\n",
      "Episode 100 finished after 10 steps with reward 10.00\n",
      "Actor loss: 0.51, Critic loss: 3.48   Total steps: 1051\n",
      "--------------------------------------------------\n",
      "Episode 200 finished after 12 steps with reward 12.00\n",
      "Actor loss: 0.36, Critic loss: 5.55   Total steps: 2019\n",
      "--------------------------------------------------\n",
      "Episode 300 finished after 8 steps with reward 8.00\n",
      "Actor loss: 0.02, Critic loss: 4.99   Total steps: 2986\n",
      "--------------------------------------------------\n",
      "Episode 400 finished after 9 steps with reward 9.00\n",
      "Actor loss: -0.03, Critic loss: 0.30   Total steps: 3968\n",
      "--------------------------------------------------\n",
      "Episode 500 finished after 9 steps with reward 9.00\n",
      "Actor loss: -0.04, Critic loss: 0.08   Total steps: 4940\n",
      "--------------------------------------------------\n",
      "Episode 600 finished after 9 steps with reward 9.00\n",
      "Actor loss: -0.01, Critic loss: 0.07   Total steps: 5903\n",
      "--------------------------------------------------\n",
      "Episode 700 finished after 8 steps with reward 8.00\n",
      "Actor loss: -0.10, Critic loss: 0.20   Total steps: 6897\n",
      "--------------------------------------------------\n",
      "Episode 800 finished after 10 steps with reward 10.00\n",
      "Actor loss: 0.00, Critic loss: 0.10   Total steps: 7912\n",
      "--------------------------------------------------\n",
      "Episode 900 finished after 10 steps with reward 10.00\n",
      "Actor loss: -0.00, Critic loss: 0.61   Total steps: 8919\n",
      "--------------------------------------------------\n",
      "Episode 1000 finished after 10 steps with reward 10.00\n",
      "Actor loss: 0.01, Critic loss: 0.42   Total steps: 9903\n",
      "--------------------------------------------------\n",
      "Episode 1100 finished after 11 steps with reward 11.00\n",
      "Actor loss: 0.06, Critic loss: 0.25   Total steps: 10867\n",
      "--------------------------------------------------\n",
      "Episode 1200 finished after 10 steps with reward 10.00\n",
      "Actor loss: 0.01, Critic loss: 0.02   Total steps: 11841\n",
      "--------------------------------------------------\n",
      "Episode 1300 finished after 9 steps with reward 9.00\n",
      "Actor loss: -0.04, Critic loss: 0.10   Total steps: 12817\n",
      "--------------------------------------------------\n",
      "Episode 1400 finished after 11 steps with reward 11.00\n",
      "Actor loss: 0.05, Critic loss: 0.35   Total steps: 13795\n",
      "--------------------------------------------------\n",
      "Episode 1500 finished after 10 steps with reward 10.00\n",
      "Actor loss: 0.02, Critic loss: 0.00   Total steps: 14769\n",
      "--------------------------------------------------\n",
      "Episode 1600 finished after 11 steps with reward 11.00\n",
      "Actor loss: 0.03, Critic loss: 0.03   Total steps: 15767\n",
      "--------------------------------------------------\n",
      "Episode 1700 finished after 8 steps with reward 8.00\n",
      "Actor loss: -0.06, Critic loss: 0.03   Total steps: 16753\n",
      "--------------------------------------------------\n",
      "Episode 1800 finished after 8 steps with reward 8.00\n",
      "Actor loss: -0.08, Critic loss: 0.13   Total steps: 17713\n",
      "--------------------------------------------------\n",
      "Episode 1900 finished after 10 steps with reward 10.00\n",
      "Actor loss: 0.02, Critic loss: 0.05   Total steps: 18706\n",
      "--------------------------------------------------\n",
      "Episode 2000 finished after 8 steps with reward 8.00\n",
      "Actor loss: -0.09, Critic loss: 0.11   Total steps: 19688\n",
      "--------------------------------------------------\n",
      "Episode 2100 finished after 11 steps with reward 11.00\n",
      "Actor loss: 0.08, Critic loss: 0.42   Total steps: 20660\n",
      "--------------------------------------------------\n",
      "Episode 2200 finished after 9 steps with reward 9.00\n",
      "Actor loss: 0.01, Critic loss: 0.00   Total steps: 21669\n",
      "--------------------------------------------------\n",
      "Episode 2300 finished after 10 steps with reward 10.00\n",
      "Actor loss: 0.00, Critic loss: 0.30   Total steps: 22658\n",
      "--------------------------------------------------\n",
      "Episode 2400 finished after 10 steps with reward 10.00\n",
      "Actor loss: 0.01, Critic loss: 0.01   Total steps: 23636\n",
      "--------------------------------------------------\n",
      "Episode 2500 finished after 10 steps with reward 10.00\n",
      "Actor loss: -0.00, Critic loss: 0.01   Total steps: 24619\n",
      "--------------------------------------------------\n",
      "Episode 2600 finished after 10 steps with reward 10.00\n",
      "Actor loss: 0.06, Critic loss: 0.21   Total steps: 25603\n",
      "--------------------------------------------------\n",
      "Episode 2700 finished after 10 steps with reward 10.00\n",
      "Actor loss: 0.03, Critic loss: 0.03   Total steps: 26582\n",
      "--------------------------------------------------\n",
      "Episode 2800 finished after 9 steps with reward 9.00\n",
      "Actor loss: -0.02, Critic loss: 0.01   Total steps: 27581\n",
      "--------------------------------------------------\n",
      "Episode 2900 finished after 12 steps with reward 12.00\n",
      "Actor loss: 0.06, Critic loss: 0.25   Total steps: 28585\n",
      "--------------------------------------------------\n",
      "Episode 3000 finished after 9 steps with reward 9.00\n",
      "Actor loss: 0.00, Critic loss: 0.00   Total steps: 29579\n",
      "--------------------------------------------------\n",
      "Episode 3100 finished after 9 steps with reward 9.00\n",
      "Actor loss: -0.02, Critic loss: 0.02   Total steps: 30565\n",
      "--------------------------------------------------\n",
      "Episode 3200 finished after 11 steps with reward 11.00\n",
      "Actor loss: 0.07, Critic loss: 0.27   Total steps: 31562\n",
      "--------------------------------------------------\n",
      "Episode 3300 finished after 10 steps with reward 10.00\n",
      "Actor loss: 0.00, Critic loss: 0.00   Total steps: 32562\n",
      "--------------------------------------------------\n",
      "Episode 3400 finished after 10 steps with reward 10.00\n",
      "Actor loss: 0.00, Critic loss: 0.01   Total steps: 33556\n",
      "--------------------------------------------------\n",
      "Episode 3500 finished after 9 steps with reward 9.00\n",
      "Actor loss: -0.04, Critic loss: 0.03   Total steps: 34528\n",
      "--------------------------------------------------\n",
      "Episode 3600 finished after 11 steps with reward 11.00\n",
      "Actor loss: 0.06, Critic loss: 0.08   Total steps: 35508\n",
      "--------------------------------------------------\n",
      "Episode 3700 finished after 13 steps with reward 13.00\n",
      "Actor loss: 0.16, Critic loss: 0.35   Total steps: 36482\n",
      "--------------------------------------------------\n",
      "Episode 3800 finished after 9 steps with reward 9.00\n",
      "Actor loss: -0.02, Critic loss: 0.03   Total steps: 37452\n",
      "--------------------------------------------------\n",
      "Episode 3900 finished after 12 steps with reward 12.00\n",
      "Actor loss: 0.10, Critic loss: 0.45   Total steps: 38454\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m next_state, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     45\u001b[0m agent\u001b[38;5;241m.\u001b[39mnum_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 47\u001b[0m next_state \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Convert next_state to a tensor\u001b[39;00m\n\u001b[1;32m     48\u001b[0m done \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n\u001b[1;32m     49\u001b[0m episode_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = device_selection() # mps -> cuda -> cpu\n",
    "\n",
    "# Initialize environment\n",
    "env = gym.make('CartPole-v1') \n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 256\n",
    "gamma_ = 0.99\n",
    "lr_actor = 1e-5\n",
    "lr_critic = 1e-3\n",
    "eps = 0.1\n",
    "num_workers=1\n",
    "num_episodes = 10000\n",
    "total_steps_budget = 500000\n",
    "max_steps_per_episode = 1000\n",
    "\n",
    "# neural network structure\n",
    "input_size = env.observation_space.shape[0] # 4\n",
    "hidden_size = 64\n",
    "output_size_actor = env.action_space.n # 2\n",
    "output_size_critic = 1\n",
    "\n",
    "critic_losses = []\n",
    "actor_losses = []\n",
    "\n",
    "\n",
    "# Initialize agent \n",
    "agent = Agent(input_size, hidden_size, \\\n",
    "                output_size_actor, output_size_critic, \\\n",
    "                eps, gamma_, lr_actor, lr_critic, num_workers, \\\n",
    "                device=device)\n",
    "\n",
    "# Initialize batch\n",
    "batch = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    state = torch.from_numpy(state).float().to(device)  # Convert state to a tensor\n",
    "\n",
    "    episode_reward = 0\n",
    "\n",
    "    for t in range(max_steps_per_episode):\n",
    "        action = agent.select_action(state, worker_id=0, policy=\"eps-greedy\")\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        agent.num_steps += 1\n",
    "        \n",
    "        next_state = torch.from_numpy(next_state).float().to(device)  # Convert next_state to a tensor\n",
    "        done = terminated or truncated\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Add the experience to the batch\n",
    "        batch.append((state, action, reward, next_state, done))\n",
    "\n",
    "        if len(batch) >= batch_size or done:\n",
    "            # this line is to make batch compatible with the training. \n",
    "            # Should be a dict with keys as worker_ids when num_workers>1\n",
    "            batches_dict = {0: batch}\n",
    "            # Train the agent\n",
    "            critic_loss, actor_loss = agent.train(batches_dict, agent.gamma, agent.lr_actor, agent.lr_critic, agent.device)\n",
    "            \n",
    "            critic_losses.append(critic_loss)\n",
    "            actor_losses.append(actor_loss)\n",
    "            # Clear the batch\n",
    "            batch.clear()\n",
    "\n",
    "        state = next_state\n",
    "        if done: break\n",
    "\n",
    "    if episode % 100 == 0:\n",
    "        print(f\"Episode {episode} finished after {t+1} steps with reward {episode_reward:.2f}\")\n",
    "        print(f\"Actor loss: {actor_loss:.2f}, Critic loss: {critic_loss:.2f}\",f\"  Total steps: {agent.num_steps}\")\n",
    "        print(\"--------------------------------------------------\")\n",
    "    \n",
    "    if (agent.num_steps >= total_steps_budget): \n",
    "        print(f\"Reached total training budget of {total_steps_budget} steps ----> Stopping training at episode {episode}\")\n",
    "        break\n",
    "\n",
    "agent.training_done()\n",
    "agent.save(\"./A2C_cartpole\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "workers all use the same parameters and get updated with the same gradient at the same time (if not at the same time --> updates with different policies by definition, but A2C is on-policy). Doesn't matter if they (inevitably) end episodes at different time-steps. What matters is they have the same policy when accumulating a batch (same actor-critic params)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batches vanilla are (1x1), in general (num_time_steps x num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv4annrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
