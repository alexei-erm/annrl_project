{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from training import *\n",
    "from models import *\n",
    "from A2C_agent import *\n",
    "from helpers import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 finished after 38 steps with reward 38.00\n",
      "Episode 100 finished after 40 steps with reward 40.00\n",
      "Episode 200 finished after 24 steps with reward 24.00\n",
      "Episode 300 finished after 44 steps with reward 44.00\n",
      "Episode 400 finished after 24 steps with reward 24.00\n",
      "Episode 500 finished after 19 steps with reward 19.00\n",
      "Episode 600 finished after 25 steps with reward 25.00\n",
      "Episode 700 finished after 30 steps with reward 30.00\n",
      "Episode 800 finished after 43 steps with reward 43.00\n",
      "Episode 900 finished after 19 steps with reward 19.00\n",
      "Episode 1000 finished after 29 steps with reward 29.00\n",
      "Episode 1100 finished after 16 steps with reward 16.00\n",
      "Episode 1200 finished after 31 steps with reward 31.00\n",
      "Episode 1300 finished after 58 steps with reward 58.00\n",
      "Episode 1400 finished after 21 steps with reward 21.00\n",
      "Episode 1500 finished after 19 steps with reward 19.00\n",
      "Episode 1600 finished after 44 steps with reward 44.00\n",
      "Episode 1700 finished after 23 steps with reward 23.00\n",
      "Episode 1800 finished after 27 steps with reward 27.00\n",
      "Episode 1900 finished after 37 steps with reward 37.00\n",
      "Episode 2000 finished after 56 steps with reward 56.00\n",
      "Episode 2100 finished after 20 steps with reward 20.00\n",
      "Episode 2200 finished after 21 steps with reward 21.00\n",
      "Episode 2300 finished after 16 steps with reward 16.00\n",
      "Episode 2400 finished after 33 steps with reward 33.00\n",
      "Episode 2500 finished after 29 steps with reward 29.00\n",
      "Episode 2600 finished after 24 steps with reward 24.00\n",
      "Episode 2700 finished after 34 steps with reward 34.00\n",
      "Episode 2800 finished after 15 steps with reward 15.00\n",
      "Episode 2900 finished after 15 steps with reward 15.00\n",
      "Episode 3000 finished after 17 steps with reward 17.00\n",
      "Episode 3100 finished after 15 steps with reward 15.00\n",
      "Episode 3200 finished after 31 steps with reward 31.00\n",
      "Episode 3300 finished after 19 steps with reward 19.00\n",
      "Episode 3400 finished after 22 steps with reward 22.00\n",
      "Episode 3500 finished after 25 steps with reward 25.00\n",
      "Episode 3600 finished after 27 steps with reward 27.00\n",
      "Episode 3700 finished after 19 steps with reward 19.00\n",
      "Episode 3800 finished after 19 steps with reward 19.00\n",
      "Episode 3900 finished after 25 steps with reward 25.00\n",
      "Episode 4000 finished after 23 steps with reward 23.00\n",
      "Episode 4100 finished after 26 steps with reward 26.00\n",
      "Episode 4200 finished after 15 steps with reward 15.00\n",
      "Episode 4300 finished after 17 steps with reward 17.00\n",
      "Episode 4400 finished after 25 steps with reward 25.00\n",
      "Episode 4500 finished after 34 steps with reward 34.00\n",
      "Episode 4600 finished after 20 steps with reward 20.00\n",
      "Episode 4700 finished after 30 steps with reward 30.00\n",
      "Episode 4800 finished after 18 steps with reward 18.00\n",
      "Episode 4900 finished after 14 steps with reward 14.00\n",
      "Episode 5000 finished after 20 steps with reward 20.00\n",
      "Episode 5100 finished after 31 steps with reward 31.00\n",
      "Episode 5200 finished after 25 steps with reward 25.00\n",
      "Episode 5300 finished after 32 steps with reward 32.00\n",
      "Episode 5400 finished after 16 steps with reward 16.00\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "gamma_ = 0.99\n",
    "lr_actor = 1e-5\n",
    "lr_critic = 1e-3\n",
    "eps = 0.1\n",
    "\n",
    "input_size=4\n",
    "hidden_size = 64\n",
    "output_size_actor=2\n",
    "output_size_critic=1\n",
    "num_workers=10\n",
    "num_episodes = 10000\n",
    "max_steps_per_episode = 1000\n",
    "\n",
    "device = device_selection() # mps -> cuda -> cpu\n",
    "\n",
    "# Initialize environment\n",
    "env = gym.make('CartPole-v1') \n",
    "\n",
    "# Initialize agent \n",
    "agent = Agent(input_size, hidden_size, \\\n",
    "                output_size_actor, output_size_critic, \\\n",
    "                eps, gamma_, lr_actor, lr_critic, num_workers, \\\n",
    "                device=device)\n",
    "\n",
    "# Initialize batch\n",
    "batch = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    state = torch.from_numpy(state).float().to(device)  # Convert state to a tensor\n",
    "\n",
    "    episode_reward = 0\n",
    "\n",
    "    for t in range(max_steps_per_episode):\n",
    "        action = agent.select_action(state, worker_id=0, policy=\"eps-greedy\")\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        \n",
    "        next_state = torch.from_numpy(next_state).float().to(device)  # Convert next_state to a tensor\n",
    "        done = terminated or truncated\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Add the experience to the batch\n",
    "        batch.append((state, action, reward, next_state, done))\n",
    "\n",
    "        if len(batch) >= batch_size :\n",
    "            # Train the agent\n",
    "            worker_losses = agent.train({0:batch}, agent.gamma, agent.lr_actor, agent.lr_critic, agent.device)\n",
    "            xx=batch\n",
    "            # Clear the batch\n",
    "            batch.clear()\n",
    "\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    if episode % 100 == 0:\n",
    "        print(f\"Episode {episode} finished after {t+1} steps with reward {episode_reward:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO IMPLEMENT:\n",
    "\n",
    "- fix training for multiple workers (wrong computations for target and advantage probably!)\n",
    "- methods for plotting and training observation (cf 2.3 of project pdf)\n",
    "- methods for saving agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([1], device='mps:0'),\n",
       " tensor([1], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([1], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([1], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([1], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([1], device='mps:0'),\n",
       " tensor([0], device='mps:0'),\n",
       " tensor([0], device='mps:0')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x[1] for x in xx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
